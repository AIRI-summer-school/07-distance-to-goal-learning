{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['WANDB_API_KEY'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "example_map = [[1, 1, 1, 1, 1],\n",
    "        [1, 'c', 0, 0, 1],\n",
    "        [1, 1, 1, 0, 1],\n",
    "        [1, 'c', 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = os.path.basename(__file__)[: -len(\".py\")] if \"__file__\" in locals() else \"ipynb_experiment\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = True\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"AIRI_RL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = True\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = 'PointMaze_UMaze-v3'\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 1000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 2048\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 32\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 10\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.0\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "def make_env(env_id, idx, capture_video, run_name, gamma):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = FlattenObservation(env)              # ← NEW: dict ➜ flat 1-D vector\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (8,), float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.single_observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dim  = int(np.prod(envs.single_observation_space.shape))\n",
    "        action_dim = int(np.prod(envs.single_action_space.shape))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(input_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(input_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
    "\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgamershmidt-sofya\u001b[0m (\u001b[33mgamershmidt-sofya-innopolis-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/user7/.venv/lib/python3.10/site-packages/wandb/analytics/sentry.py:258: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "/home/user7/.venv/lib/python3.10/site-packages/wandb/analytics/sentry.py:258: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user7/07-distance-to-goal-learning/PPO_Clean_RL_PointMaze/wandb/run-20250708_211456-xn4xyacd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gamershmidt-sofya-innopolis-university/AIRI_RL/runs/xn4xyacd' target=\"_blank\">PointMaze_UMaze-v3__ipynb_experiment__1__1751998495</a></strong> to <a href='https://wandb.ai/gamershmidt-sofya-innopolis-university/AIRI_RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gamershmidt-sofya-innopolis-university/AIRI_RL' target=\"_blank\">https://wandb.ai/gamershmidt-sofya-innopolis-university/AIRI_RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gamershmidt-sofya-innopolis-university/AIRI_RL/runs/xn4xyacd' target=\"_blank\">https://wandb.ai/gamershmidt-sofya-innopolis-university/AIRI_RL/runs/xn4xyacd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Args()\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=args.seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=300, episodic_return=[0.]\n",
      "global_step=600, episodic_return=[128.]\n",
      "global_step=900, episodic_return=[0.]\n",
      "global_step=1200, episodic_return=[0.]\n",
      "global_step=1500, episodic_return=[204.]\n",
      "global_step=1800, episodic_return=[36.]\n",
      "SPS: 162\n",
      "global_step=2100, episodic_return=[184.]\n",
      "global_step=2400, episodic_return=[0.]\n",
      "global_step=2700, episodic_return=[0.]\n",
      "global_step=3000, episodic_return=[0.]\n",
      "global_step=3300, episodic_return=[0.]\n",
      "global_step=3600, episodic_return=[0.]\n",
      "global_step=3900, episodic_return=[0.]\n",
      "SPS: 236\n",
      "global_step=4200, episodic_return=[0.]\n",
      "global_step=4500, episodic_return=[30.]\n",
      "global_step=4800, episodic_return=[0.]\n",
      "global_step=5100, episodic_return=[35.]\n",
      "global_step=5400, episodic_return=[39.]\n",
      "global_step=5700, episodic_return=[0.]\n",
      "global_step=6000, episodic_return=[0.]\n",
      "SPS: 280\n",
      "global_step=6300, episodic_return=[0.]\n",
      "global_step=6600, episodic_return=[24.]\n",
      "global_step=6900, episodic_return=[0.]\n",
      "global_step=7200, episodic_return=[34.]\n",
      "global_step=7500, episodic_return=[0.]\n",
      "global_step=7800, episodic_return=[35.]\n",
      "global_step=8100, episodic_return=[52.]\n",
      "SPS: 309\n",
      "global_step=8400, episodic_return=[72.]\n",
      "global_step=8700, episodic_return=[0.]\n",
      "global_step=9000, episodic_return=[0.]\n",
      "global_step=9300, episodic_return=[146.]\n",
      "global_step=9600, episodic_return=[0.]\n",
      "global_step=9900, episodic_return=[30.]\n",
      "global_step=10200, episodic_return=[0.]\n",
      "SPS: 329\n",
      "global_step=10500, episodic_return=[39.]\n",
      "global_step=10800, episodic_return=[164.]\n",
      "global_step=11100, episodic_return=[0.]\n",
      "global_step=11400, episodic_return=[0.]\n",
      "global_step=11700, episodic_return=[60.]\n",
      "global_step=12000, episodic_return=[0.]\n",
      "SPS: 345\n",
      "global_step=12300, episodic_return=[0.]\n",
      "global_step=12600, episodic_return=[0.]\n",
      "global_step=12900, episodic_return=[0.]\n",
      "global_step=13200, episodic_return=[0.]\n",
      "global_step=13500, episodic_return=[0.]\n",
      "global_step=13800, episodic_return=[0.]\n",
      "global_step=14100, episodic_return=[59.]\n",
      "SPS: 357\n",
      "global_step=14400, episodic_return=[0.]\n",
      "global_step=14700, episodic_return=[108.]\n",
      "global_step=15000, episodic_return=[0.]\n",
      "global_step=15300, episodic_return=[0.]\n",
      "global_step=15600, episodic_return=[0.]\n",
      "global_step=15900, episodic_return=[60.]\n",
      "global_step=16200, episodic_return=[26.]\n",
      "SPS: 367\n",
      "global_step=16500, episodic_return=[0.]\n",
      "global_step=16800, episodic_return=[7.]\n",
      "global_step=17100, episodic_return=[0.]\n",
      "global_step=17400, episodic_return=[0.]\n",
      "global_step=17700, episodic_return=[45.]\n",
      "global_step=18000, episodic_return=[0.]\n",
      "global_step=18300, episodic_return=[41.]\n",
      "SPS: 374\n",
      "global_step=18600, episodic_return=[0.]\n",
      "global_step=18900, episodic_return=[0.]\n",
      "global_step=19200, episodic_return=[0.]\n",
      "global_step=19500, episodic_return=[0.]\n",
      "global_step=19800, episodic_return=[37.]\n",
      "global_step=20100, episodic_return=[0.]\n",
      "global_step=20400, episodic_return=[0.]\n",
      "SPS: 381\n",
      "global_step=20700, episodic_return=[93.]\n",
      "global_step=21000, episodic_return=[65.]\n",
      "global_step=21300, episodic_return=[44.]\n",
      "global_step=21600, episodic_return=[0.]\n",
      "global_step=21900, episodic_return=[161.]\n",
      "global_step=22200, episodic_return=[98.]\n",
      "global_step=22500, episodic_return=[85.]\n",
      "SPS: 387\n",
      "global_step=22800, episodic_return=[0.]\n",
      "global_step=23100, episodic_return=[0.]\n",
      "global_step=23400, episodic_return=[0.]\n",
      "global_step=23700, episodic_return=[129.]\n",
      "global_step=24000, episodic_return=[42.]\n",
      "global_step=24300, episodic_return=[69.]\n",
      "SPS: 391\n",
      "global_step=24600, episodic_return=[0.]\n",
      "global_step=24900, episodic_return=[0.]\n",
      "global_step=25200, episodic_return=[0.]\n",
      "global_step=25500, episodic_return=[65.]\n",
      "global_step=25800, episodic_return=[79.]\n",
      "global_step=26100, episodic_return=[0.]\n",
      "global_step=26400, episodic_return=[18.]\n",
      "SPS: 395\n",
      "global_step=26700, episodic_return=[0.]\n",
      "global_step=27000, episodic_return=[0.]\n",
      "global_step=27300, episodic_return=[0.]\n",
      "global_step=27600, episodic_return=[0.]\n",
      "global_step=27900, episodic_return=[0.]\n",
      "global_step=28200, episodic_return=[0.]\n",
      "global_step=28500, episodic_return=[87.]\n",
      "SPS: 399\n",
      "global_step=28800, episodic_return=[87.]\n",
      "global_step=29100, episodic_return=[0.]\n",
      "global_step=29400, episodic_return=[0.]\n",
      "global_step=29700, episodic_return=[0.]\n",
      "global_step=30000, episodic_return=[97.]\n",
      "global_step=30300, episodic_return=[121.]\n",
      "global_step=30600, episodic_return=[0.]\n",
      "SPS: 402\n",
      "global_step=30900, episodic_return=[93.]\n",
      "global_step=31200, episodic_return=[0.]\n",
      "global_step=31500, episodic_return=[0.]\n",
      "global_step=31800, episodic_return=[0.]\n",
      "global_step=32100, episodic_return=[0.]\n",
      "global_step=32400, episodic_return=[0.]\n",
      "global_step=32700, episodic_return=[48.]\n",
      "SPS: 405\n",
      "global_step=33000, episodic_return=[0.]\n",
      "global_step=33300, episodic_return=[0.]\n",
      "global_step=33600, episodic_return=[0.]\n",
      "global_step=33900, episodic_return=[188.]\n",
      "global_step=34200, episodic_return=[0.]\n",
      "global_step=34500, episodic_return=[0.]\n",
      "global_step=34800, episodic_return=[0.]\n",
      "SPS: 407\n",
      "global_step=35100, episodic_return=[28.]\n",
      "global_step=35400, episodic_return=[0.]\n",
      "global_step=35700, episodic_return=[0.]\n",
      "global_step=36000, episodic_return=[0.]\n",
      "global_step=36300, episodic_return=[175.]\n",
      "global_step=36600, episodic_return=[0.]\n",
      "SPS: 410\n",
      "global_step=36900, episodic_return=[0.]\n",
      "global_step=37200, episodic_return=[65.]\n",
      "global_step=37500, episodic_return=[0.]\n",
      "global_step=37800, episodic_return=[0.]\n",
      "global_step=38100, episodic_return=[0.]\n",
      "global_step=38400, episodic_return=[138.]\n",
      "global_step=38700, episodic_return=[0.]\n",
      "SPS: 412\n",
      "global_step=39000, episodic_return=[84.]\n",
      "global_step=39300, episodic_return=[0.]\n",
      "global_step=39600, episodic_return=[115.]\n",
      "global_step=39900, episodic_return=[0.]\n",
      "global_step=40200, episodic_return=[233.]\n",
      "global_step=40500, episodic_return=[0.]\n",
      "global_step=40800, episodic_return=[0.]\n",
      "SPS: 413\n",
      "global_step=41100, episodic_return=[21.]\n",
      "global_step=41400, episodic_return=[0.]\n",
      "global_step=41700, episodic_return=[125.]\n",
      "global_step=42000, episodic_return=[0.]\n",
      "global_step=42300, episodic_return=[231.]\n",
      "global_step=42600, episodic_return=[144.]\n",
      "global_step=42900, episodic_return=[0.]\n",
      "SPS: 415\n",
      "global_step=43200, episodic_return=[0.]\n",
      "global_step=43500, episodic_return=[270.]\n",
      "global_step=43800, episodic_return=[0.]\n",
      "global_step=44100, episodic_return=[0.]\n",
      "global_step=44400, episodic_return=[0.]\n",
      "global_step=44700, episodic_return=[101.]\n",
      "global_step=45000, episodic_return=[0.]\n",
      "SPS: 417\n",
      "global_step=45300, episodic_return=[0.]\n",
      "global_step=45600, episodic_return=[87.]\n",
      "global_step=45900, episodic_return=[0.]\n",
      "global_step=46200, episodic_return=[151.]\n",
      "global_step=46500, episodic_return=[0.]\n",
      "global_step=46800, episodic_return=[0.]\n",
      "global_step=47100, episodic_return=[147.]\n",
      "SPS: 418\n",
      "global_step=47400, episodic_return=[0.]\n",
      "global_step=47700, episodic_return=[0.]\n",
      "global_step=48000, episodic_return=[0.]\n",
      "global_step=48300, episodic_return=[0.]\n",
      "global_step=48600, episodic_return=[0.]\n",
      "global_step=48900, episodic_return=[0.]\n",
      "SPS: 419\n",
      "global_step=49200, episodic_return=[0.]\n",
      "global_step=49500, episodic_return=[117.]\n",
      "global_step=49800, episodic_return=[0.]\n",
      "global_step=50100, episodic_return=[140.]\n",
      "global_step=50400, episodic_return=[0.]\n",
      "global_step=50700, episodic_return=[0.]\n",
      "global_step=51000, episodic_return=[0.]\n",
      "SPS: 421\n",
      "global_step=51300, episodic_return=[0.]\n",
      "global_step=51600, episodic_return=[0.]\n",
      "global_step=51900, episodic_return=[0.]\n",
      "global_step=52200, episodic_return=[0.]\n",
      "global_step=52500, episodic_return=[0.]\n",
      "global_step=52800, episodic_return=[34.]\n",
      "global_step=53100, episodic_return=[0.]\n",
      "SPS: 422\n",
      "global_step=53400, episodic_return=[0.]\n",
      "global_step=53700, episodic_return=[61.]\n",
      "global_step=54000, episodic_return=[8.]\n",
      "global_step=54300, episodic_return=[20.]\n",
      "global_step=54600, episodic_return=[0.]\n",
      "global_step=54900, episodic_return=[0.]\n",
      "global_step=55200, episodic_return=[87.]\n",
      "SPS: 423\n",
      "global_step=55500, episodic_return=[78.]\n",
      "global_step=55800, episodic_return=[156.]\n",
      "global_step=56100, episodic_return=[0.]\n",
      "global_step=56400, episodic_return=[0.]\n",
      "global_step=56700, episodic_return=[0.]\n",
      "global_step=57000, episodic_return=[0.]\n",
      "global_step=57300, episodic_return=[0.]\n",
      "SPS: 424\n",
      "global_step=57600, episodic_return=[118.]\n",
      "global_step=57900, episodic_return=[0.]\n",
      "global_step=58200, episodic_return=[0.]\n",
      "global_step=58500, episodic_return=[0.]\n",
      "global_step=58800, episodic_return=[31.]\n",
      "global_step=59100, episodic_return=[72.]\n",
      "SPS: 425\n",
      "global_step=59400, episodic_return=[0.]\n",
      "global_step=59700, episodic_return=[0.]\n",
      "global_step=60000, episodic_return=[249.]\n",
      "global_step=60300, episodic_return=[0.]\n",
      "global_step=60600, episodic_return=[82.]\n",
      "global_step=60900, episodic_return=[0.]\n",
      "global_step=61200, episodic_return=[0.]\n",
      "SPS: 426\n",
      "global_step=61500, episodic_return=[26.]\n",
      "global_step=61800, episodic_return=[0.]\n",
      "global_step=62100, episodic_return=[21.]\n",
      "global_step=62400, episodic_return=[0.]\n",
      "global_step=62700, episodic_return=[0.]\n",
      "global_step=63000, episodic_return=[88.]\n",
      "global_step=63300, episodic_return=[0.]\n",
      "SPS: 427\n",
      "global_step=63600, episodic_return=[0.]\n",
      "global_step=63900, episodic_return=[0.]\n",
      "global_step=64200, episodic_return=[0.]\n",
      "global_step=64500, episodic_return=[105.]\n",
      "global_step=64800, episodic_return=[0.]\n",
      "global_step=65100, episodic_return=[40.]\n",
      "global_step=65400, episodic_return=[0.]\n",
      "SPS: 428\n",
      "global_step=65700, episodic_return=[0.]\n",
      "global_step=66000, episodic_return=[0.]\n",
      "global_step=66300, episodic_return=[0.]\n",
      "global_step=66600, episodic_return=[0.]\n",
      "global_step=66900, episodic_return=[0.]\n",
      "global_step=67200, episodic_return=[0.]\n",
      "global_step=67500, episodic_return=[56.]\n",
      "SPS: 428\n",
      "global_step=67800, episodic_return=[0.]\n",
      "global_step=68100, episodic_return=[58.]\n",
      "global_step=68400, episodic_return=[0.]\n",
      "global_step=68700, episodic_return=[0.]\n",
      "global_step=69000, episodic_return=[0.]\n",
      "global_step=69300, episodic_return=[0.]\n",
      "global_step=69600, episodic_return=[0.]\n",
      "SPS: 429\n",
      "global_step=69900, episodic_return=[0.]\n",
      "global_step=70200, episodic_return=[0.]\n",
      "global_step=70500, episodic_return=[0.]\n",
      "global_step=70800, episodic_return=[13.]\n",
      "global_step=71100, episodic_return=[43.]\n",
      "global_step=71400, episodic_return=[0.]\n",
      "SPS: 430\n",
      "global_step=71700, episodic_return=[0.]\n",
      "global_step=72000, episodic_return=[104.]\n",
      "global_step=72300, episodic_return=[0.]\n",
      "global_step=72600, episodic_return=[29.]\n",
      "global_step=72900, episodic_return=[0.]\n",
      "global_step=73200, episodic_return=[28.]\n",
      "global_step=73500, episodic_return=[52.]\n",
      "SPS: 431\n",
      "global_step=73800, episodic_return=[97.]\n",
      "global_step=74100, episodic_return=[0.]\n",
      "global_step=74400, episodic_return=[0.]\n",
      "global_step=74700, episodic_return=[0.]\n",
      "global_step=75000, episodic_return=[28.]\n",
      "global_step=75300, episodic_return=[0.]\n",
      "global_step=75600, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=75900, episodic_return=[0.]\n",
      "global_step=76200, episodic_return=[0.]\n",
      "global_step=76500, episodic_return=[34.]\n",
      "global_step=76800, episodic_return=[0.]\n",
      "global_step=77100, episodic_return=[0.]\n",
      "global_step=77400, episodic_return=[109.]\n",
      "global_step=77700, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=78000, episodic_return=[139.]\n",
      "global_step=78300, episodic_return=[0.]\n",
      "global_step=78600, episodic_return=[80.]\n",
      "global_step=78900, episodic_return=[76.]\n",
      "global_step=79200, episodic_return=[101.]\n",
      "global_step=79500, episodic_return=[0.]\n",
      "global_step=79800, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=80100, episodic_return=[0.]\n",
      "global_step=80400, episodic_return=[126.]\n",
      "global_step=80700, episodic_return=[128.]\n",
      "global_step=81000, episodic_return=[0.]\n",
      "global_step=81300, episodic_return=[0.]\n",
      "global_step=81600, episodic_return=[0.]\n",
      "global_step=81900, episodic_return=[163.]\n",
      "SPS: 434\n",
      "global_step=82200, episodic_return=[0.]\n",
      "global_step=82500, episodic_return=[0.]\n",
      "global_step=82800, episodic_return=[0.]\n",
      "global_step=83100, episodic_return=[251.]\n",
      "global_step=83400, episodic_return=[0.]\n",
      "global_step=83700, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=84000, episodic_return=[0.]\n",
      "global_step=84300, episodic_return=[126.]\n",
      "global_step=84600, episodic_return=[68.]\n",
      "global_step=84900, episodic_return=[25.]\n",
      "global_step=85200, episodic_return=[0.]\n",
      "global_step=85500, episodic_return=[0.]\n",
      "global_step=85800, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=86100, episodic_return=[179.]\n",
      "global_step=86400, episodic_return=[92.]\n",
      "global_step=86700, episodic_return=[0.]\n",
      "global_step=87000, episodic_return=[0.]\n",
      "global_step=87300, episodic_return=[23.]\n",
      "global_step=87600, episodic_return=[0.]\n",
      "global_step=87900, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=88200, episodic_return=[0.]\n",
      "global_step=88500, episodic_return=[0.]\n",
      "global_step=88800, episodic_return=[158.]\n",
      "global_step=89100, episodic_return=[86.]\n",
      "global_step=89400, episodic_return=[259.]\n",
      "global_step=89700, episodic_return=[0.]\n",
      "global_step=90000, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=90300, episodic_return=[0.]\n",
      "global_step=90600, episodic_return=[0.]\n",
      "global_step=90900, episodic_return=[150.]\n",
      "global_step=91200, episodic_return=[0.]\n",
      "global_step=91500, episodic_return=[0.]\n",
      "global_step=91800, episodic_return=[0.]\n",
      "global_step=92100, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=92400, episodic_return=[58.]\n",
      "global_step=92700, episodic_return=[190.]\n",
      "global_step=93000, episodic_return=[13.]\n",
      "global_step=93300, episodic_return=[0.]\n",
      "global_step=93600, episodic_return=[0.]\n",
      "global_step=93900, episodic_return=[0.]\n",
      "global_step=94200, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=94500, episodic_return=[0.]\n",
      "global_step=94800, episodic_return=[120.]\n",
      "global_step=95100, episodic_return=[0.]\n",
      "global_step=95400, episodic_return=[0.]\n",
      "global_step=95700, episodic_return=[0.]\n",
      "global_step=96000, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=96300, episodic_return=[0.]\n",
      "global_step=96600, episodic_return=[216.]\n",
      "global_step=96900, episodic_return=[0.]\n",
      "global_step=97200, episodic_return=[0.]\n",
      "global_step=97500, episodic_return=[0.]\n",
      "global_step=97800, episodic_return=[0.]\n",
      "global_step=98100, episodic_return=[0.]\n",
      "SPS: 437\n",
      "global_step=98400, episodic_return=[0.]\n",
      "global_step=98700, episodic_return=[0.]\n",
      "global_step=99000, episodic_return=[0.]\n",
      "global_step=99300, episodic_return=[0.]\n",
      "global_step=99600, episodic_return=[202.]\n",
      "global_step=99900, episodic_return=[0.]\n",
      "global_step=100200, episodic_return=[0.]\n",
      "SPS: 437\n",
      "global_step=100500, episodic_return=[0.]\n",
      "global_step=100800, episodic_return=[0.]\n",
      "global_step=101100, episodic_return=[0.]\n",
      "global_step=101400, episodic_return=[268.]\n",
      "global_step=101700, episodic_return=[61.]\n",
      "global_step=102000, episodic_return=[0.]\n",
      "global_step=102300, episodic_return=[61.]\n",
      "SPS: 437\n",
      "global_step=102600, episodic_return=[278.]\n",
      "global_step=102900, episodic_return=[0.]\n",
      "global_step=103200, episodic_return=[222.]\n",
      "global_step=103500, episodic_return=[0.]\n",
      "global_step=103800, episodic_return=[0.]\n",
      "global_step=104100, episodic_return=[0.]\n",
      "global_step=104400, episodic_return=[0.]\n",
      "SPS: 437\n",
      "global_step=104700, episodic_return=[0.]\n",
      "global_step=105000, episodic_return=[172.]\n",
      "global_step=105300, episodic_return=[189.]\n",
      "global_step=105600, episodic_return=[112.]\n",
      "global_step=105900, episodic_return=[0.]\n",
      "global_step=106200, episodic_return=[0.]\n",
      "SPS: 437\n",
      "global_step=106500, episodic_return=[237.]\n",
      "global_step=106800, episodic_return=[280.]\n",
      "global_step=107100, episodic_return=[0.]\n",
      "global_step=107400, episodic_return=[0.]\n",
      "global_step=107700, episodic_return=[0.]\n",
      "global_step=108000, episodic_return=[0.]\n",
      "global_step=108300, episodic_return=[278.]\n",
      "SPS: 437\n",
      "global_step=108600, episodic_return=[214.]\n",
      "global_step=108900, episodic_return=[0.]\n",
      "global_step=109200, episodic_return=[0.]\n",
      "global_step=109500, episodic_return=[269.]\n",
      "global_step=109800, episodic_return=[0.]\n",
      "global_step=110100, episodic_return=[0.]\n",
      "global_step=110400, episodic_return=[90.]\n",
      "SPS: 437\n",
      "global_step=110700, episodic_return=[0.]\n",
      "global_step=111000, episodic_return=[0.]\n",
      "global_step=111300, episodic_return=[0.]\n",
      "global_step=111600, episodic_return=[0.]\n",
      "global_step=111900, episodic_return=[279.]\n",
      "global_step=112200, episodic_return=[0.]\n",
      "global_step=112500, episodic_return=[0.]\n",
      "SPS: 437\n",
      "global_step=112800, episodic_return=[0.]\n",
      "global_step=113100, episodic_return=[0.]\n",
      "global_step=113400, episodic_return=[269.]\n",
      "global_step=113700, episodic_return=[0.]\n",
      "global_step=114000, episodic_return=[10.]\n",
      "global_step=114300, episodic_return=[0.]\n",
      "global_step=114600, episodic_return=[163.]\n",
      "SPS: 437\n",
      "global_step=114900, episodic_return=[0.]\n",
      "global_step=115200, episodic_return=[0.]\n",
      "global_step=115500, episodic_return=[0.]\n",
      "global_step=115800, episodic_return=[0.]\n",
      "global_step=116100, episodic_return=[0.]\n",
      "global_step=116400, episodic_return=[137.]\n",
      "global_step=116700, episodic_return=[100.]\n",
      "SPS: 437\n",
      "global_step=117000, episodic_return=[252.]\n",
      "global_step=117300, episodic_return=[150.]\n",
      "global_step=117600, episodic_return=[278.]\n",
      "global_step=117900, episodic_return=[275.]\n",
      "global_step=118200, episodic_return=[0.]\n",
      "global_step=118500, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=118800, episodic_return=[0.]\n",
      "global_step=119100, episodic_return=[0.]\n",
      "global_step=119400, episodic_return=[0.]\n",
      "global_step=119700, episodic_return=[0.]\n",
      "global_step=120000, episodic_return=[215.]\n",
      "global_step=120300, episodic_return=[0.]\n",
      "global_step=120600, episodic_return=[136.]\n",
      "SPS: 436\n",
      "global_step=120900, episodic_return=[0.]\n",
      "global_step=121200, episodic_return=[254.]\n",
      "global_step=121500, episodic_return=[0.]\n",
      "global_step=121800, episodic_return=[81.]\n",
      "global_step=122100, episodic_return=[67.]\n",
      "global_step=122400, episodic_return=[217.]\n",
      "global_step=122700, episodic_return=[154.]\n",
      "SPS: 437\n",
      "global_step=123000, episodic_return=[0.]\n",
      "global_step=123300, episodic_return=[0.]\n",
      "global_step=123600, episodic_return=[246.]\n",
      "global_step=123900, episodic_return=[0.]\n",
      "global_step=124200, episodic_return=[0.]\n",
      "global_step=124500, episodic_return=[0.]\n",
      "global_step=124800, episodic_return=[180.]\n",
      "SPS: 437\n",
      "global_step=125100, episodic_return=[81.]\n",
      "global_step=125400, episodic_return=[280.]\n",
      "global_step=125700, episodic_return=[0.]\n",
      "global_step=126000, episodic_return=[276.]\n",
      "global_step=126300, episodic_return=[0.]\n",
      "global_step=126600, episodic_return=[267.]\n",
      "global_step=126900, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=127200, episodic_return=[22.]\n",
      "global_step=127500, episodic_return=[0.]\n",
      "global_step=127800, episodic_return=[0.]\n",
      "global_step=128100, episodic_return=[0.]\n",
      "global_step=128400, episodic_return=[79.]\n",
      "global_step=128700, episodic_return=[0.]\n",
      "global_step=129000, episodic_return=[203.]\n",
      "SPS: 435\n",
      "global_step=129300, episodic_return=[221.]\n",
      "global_step=129600, episodic_return=[0.]\n",
      "global_step=129900, episodic_return=[167.]\n",
      "global_step=130200, episodic_return=[0.]\n",
      "global_step=130500, episodic_return=[0.]\n",
      "global_step=130800, episodic_return=[87.]\n",
      "SPS: 435\n",
      "global_step=131100, episodic_return=[0.]\n",
      "global_step=131400, episodic_return=[0.]\n",
      "global_step=131700, episodic_return=[285.]\n",
      "global_step=132000, episodic_return=[268.]\n",
      "global_step=132300, episodic_return=[29.]\n",
      "global_step=132600, episodic_return=[0.]\n",
      "global_step=132900, episodic_return=[195.]\n",
      "SPS: 436\n",
      "global_step=133200, episodic_return=[0.]\n",
      "global_step=133500, episodic_return=[0.]\n",
      "global_step=133800, episodic_return=[183.]\n",
      "global_step=134100, episodic_return=[272.]\n",
      "global_step=134400, episodic_return=[130.]\n",
      "global_step=134700, episodic_return=[0.]\n",
      "global_step=135000, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=135300, episodic_return=[0.]\n",
      "global_step=135600, episodic_return=[24.]\n",
      "global_step=135900, episodic_return=[0.]\n",
      "global_step=136200, episodic_return=[0.]\n",
      "global_step=136500, episodic_return=[203.]\n",
      "global_step=136800, episodic_return=[0.]\n",
      "global_step=137100, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=137400, episodic_return=[0.]\n",
      "global_step=137700, episodic_return=[0.]\n",
      "global_step=138000, episodic_return=[0.]\n",
      "global_step=138300, episodic_return=[0.]\n",
      "global_step=138600, episodic_return=[278.]\n",
      "global_step=138900, episodic_return=[0.]\n",
      "global_step=139200, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=139500, episodic_return=[103.]\n",
      "global_step=139800, episodic_return=[129.]\n",
      "global_step=140100, episodic_return=[0.]\n",
      "global_step=140400, episodic_return=[234.]\n",
      "global_step=140700, episodic_return=[184.]\n",
      "global_step=141000, episodic_return=[0.]\n",
      "global_step=141300, episodic_return=[123.]\n",
      "SPS: 435\n",
      "global_step=141600, episodic_return=[266.]\n",
      "global_step=141900, episodic_return=[275.]\n",
      "global_step=142200, episodic_return=[0.]\n",
      "global_step=142500, episodic_return=[0.]\n",
      "global_step=142800, episodic_return=[130.]\n",
      "global_step=143100, episodic_return=[214.]\n",
      "SPS: 434\n",
      "global_step=143400, episodic_return=[0.]\n",
      "global_step=143700, episodic_return=[223.]\n",
      "global_step=144000, episodic_return=[283.]\n",
      "global_step=144300, episodic_return=[108.]\n",
      "global_step=144600, episodic_return=[0.]\n",
      "global_step=144900, episodic_return=[0.]\n",
      "global_step=145200, episodic_return=[170.]\n",
      "SPS: 434\n",
      "global_step=145500, episodic_return=[175.]\n",
      "global_step=145800, episodic_return=[260.]\n",
      "global_step=146100, episodic_return=[46.]\n",
      "global_step=146400, episodic_return=[20.]\n",
      "global_step=146700, episodic_return=[0.]\n",
      "global_step=147000, episodic_return=[0.]\n",
      "global_step=147300, episodic_return=[276.]\n",
      "SPS: 434\n",
      "global_step=147600, episodic_return=[0.]\n",
      "global_step=147900, episodic_return=[0.]\n",
      "global_step=148200, episodic_return=[0.]\n",
      "global_step=148500, episodic_return=[271.]\n",
      "global_step=148800, episodic_return=[162.]\n",
      "global_step=149100, episodic_return=[156.]\n",
      "global_step=149400, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=149700, episodic_return=[127.]\n",
      "global_step=150000, episodic_return=[26.]\n",
      "global_step=150300, episodic_return=[0.]\n",
      "global_step=150600, episodic_return=[0.]\n",
      "global_step=150900, episodic_return=[0.]\n",
      "global_step=151200, episodic_return=[281.]\n",
      "global_step=151500, episodic_return=[192.]\n",
      "SPS: 434\n",
      "global_step=151800, episodic_return=[99.]\n",
      "global_step=152100, episodic_return=[251.]\n",
      "global_step=152400, episodic_return=[0.]\n",
      "global_step=152700, episodic_return=[0.]\n",
      "global_step=153000, episodic_return=[264.]\n",
      "global_step=153300, episodic_return=[0.]\n",
      "global_step=153600, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=153900, episodic_return=[27.]\n",
      "global_step=154200, episodic_return=[109.]\n",
      "global_step=154500, episodic_return=[276.]\n",
      "global_step=154800, episodic_return=[191.]\n",
      "global_step=155100, episodic_return=[196.]\n",
      "global_step=155400, episodic_return=[87.]\n",
      "SPS: 435\n",
      "global_step=155700, episodic_return=[273.]\n",
      "global_step=156000, episodic_return=[252.]\n",
      "global_step=156300, episodic_return=[214.]\n",
      "global_step=156600, episodic_return=[260.]\n",
      "global_step=156900, episodic_return=[0.]\n",
      "global_step=157200, episodic_return=[148.]\n",
      "global_step=157500, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=157800, episodic_return=[259.]\n",
      "global_step=158100, episodic_return=[0.]\n",
      "global_step=158400, episodic_return=[0.]\n",
      "global_step=158700, episodic_return=[105.]\n",
      "global_step=159000, episodic_return=[42.]\n",
      "global_step=159300, episodic_return=[0.]\n",
      "global_step=159600, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=159900, episodic_return=[272.]\n",
      "global_step=160200, episodic_return=[0.]\n",
      "global_step=160500, episodic_return=[124.]\n",
      "global_step=160800, episodic_return=[255.]\n",
      "global_step=161100, episodic_return=[0.]\n",
      "global_step=161400, episodic_return=[183.]\n",
      "global_step=161700, episodic_return=[194.]\n",
      "SPS: 435\n",
      "global_step=162000, episodic_return=[213.]\n",
      "global_step=162300, episodic_return=[108.]\n",
      "global_step=162600, episodic_return=[126.]\n",
      "global_step=162900, episodic_return=[0.]\n",
      "global_step=163200, episodic_return=[0.]\n",
      "global_step=163500, episodic_return=[0.]\n",
      "global_step=163800, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=164100, episodic_return=[227.]\n",
      "global_step=164400, episodic_return=[132.]\n",
      "global_step=164700, episodic_return=[0.]\n",
      "global_step=165000, episodic_return=[163.]\n",
      "global_step=165300, episodic_return=[136.]\n",
      "global_step=165600, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=165900, episodic_return=[0.]\n",
      "global_step=166200, episodic_return=[0.]\n",
      "global_step=166500, episodic_return=[0.]\n",
      "global_step=166800, episodic_return=[0.]\n",
      "global_step=167100, episodic_return=[0.]\n",
      "global_step=167400, episodic_return=[0.]\n",
      "global_step=167700, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=168000, episodic_return=[0.]\n",
      "global_step=168300, episodic_return=[191.]\n",
      "global_step=168600, episodic_return=[0.]\n",
      "global_step=168900, episodic_return=[0.]\n",
      "global_step=169200, episodic_return=[106.]\n",
      "global_step=169500, episodic_return=[242.]\n",
      "global_step=169800, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=170100, episodic_return=[234.]\n",
      "global_step=170400, episodic_return=[205.]\n",
      "global_step=170700, episodic_return=[174.]\n",
      "global_step=171000, episodic_return=[218.]\n",
      "global_step=171300, episodic_return=[102.]\n",
      "global_step=171600, episodic_return=[184.]\n",
      "global_step=171900, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=172200, episodic_return=[201.]\n",
      "global_step=172500, episodic_return=[0.]\n",
      "global_step=172800, episodic_return=[0.]\n",
      "global_step=173100, episodic_return=[207.]\n",
      "global_step=173400, episodic_return=[230.]\n",
      "global_step=173700, episodic_return=[0.]\n",
      "global_step=174000, episodic_return=[124.]\n",
      "SPS: 436\n",
      "global_step=174300, episodic_return=[0.]\n",
      "global_step=174600, episodic_return=[206.]\n",
      "global_step=174900, episodic_return=[146.]\n",
      "global_step=175200, episodic_return=[0.]\n",
      "global_step=175500, episodic_return=[250.]\n",
      "global_step=175800, episodic_return=[149.]\n",
      "global_step=176100, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=176400, episodic_return=[0.]\n",
      "global_step=176700, episodic_return=[262.]\n",
      "global_step=177000, episodic_return=[0.]\n",
      "global_step=177300, episodic_return=[209.]\n",
      "global_step=177600, episodic_return=[0.]\n",
      "global_step=177900, episodic_return=[190.]\n",
      "SPS: 436\n",
      "global_step=178200, episodic_return=[0.]\n",
      "global_step=178500, episodic_return=[0.]\n",
      "global_step=178800, episodic_return=[0.]\n",
      "global_step=179100, episodic_return=[0.]\n",
      "global_step=179400, episodic_return=[218.]\n",
      "global_step=179700, episodic_return=[210.]\n",
      "global_step=180000, episodic_return=[0.]\n",
      "SPS: 436\n",
      "global_step=180300, episodic_return=[0.]\n",
      "global_step=180600, episodic_return=[202.]\n",
      "global_step=180900, episodic_return=[176.]\n",
      "global_step=181200, episodic_return=[0.]\n",
      "global_step=181500, episodic_return=[0.]\n",
      "global_step=181800, episodic_return=[188.]\n",
      "global_step=182100, episodic_return=[230.]\n",
      "SPS: 435\n",
      "global_step=182400, episodic_return=[159.]\n",
      "global_step=182700, episodic_return=[208.]\n",
      "global_step=183000, episodic_return=[0.]\n",
      "global_step=183300, episodic_return=[0.]\n",
      "global_step=183600, episodic_return=[0.]\n",
      "global_step=183900, episodic_return=[0.]\n",
      "global_step=184200, episodic_return=[134.]\n",
      "SPS: 434\n",
      "global_step=184500, episodic_return=[112.]\n",
      "global_step=184800, episodic_return=[0.]\n",
      "global_step=185100, episodic_return=[269.]\n",
      "global_step=185400, episodic_return=[222.]\n",
      "global_step=185700, episodic_return=[272.]\n",
      "global_step=186000, episodic_return=[240.]\n",
      "global_step=186300, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=186600, episodic_return=[0.]\n",
      "global_step=186900, episodic_return=[0.]\n",
      "global_step=187200, episodic_return=[21.]\n",
      "global_step=187500, episodic_return=[217.]\n",
      "global_step=187800, episodic_return=[260.]\n",
      "global_step=188100, episodic_return=[149.]\n",
      "global_step=188400, episodic_return=[27.]\n",
      "SPS: 432\n",
      "global_step=188700, episodic_return=[193.]\n",
      "global_step=189000, episodic_return=[187.]\n",
      "global_step=189300, episodic_return=[0.]\n",
      "global_step=189600, episodic_return=[0.]\n",
      "global_step=189900, episodic_return=[0.]\n",
      "global_step=190200, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=190500, episodic_return=[239.]\n",
      "global_step=190800, episodic_return=[0.]\n",
      "global_step=191100, episodic_return=[214.]\n",
      "global_step=191400, episodic_return=[0.]\n",
      "global_step=191700, episodic_return=[0.]\n",
      "global_step=192000, episodic_return=[232.]\n",
      "global_step=192300, episodic_return=[0.]\n",
      "SPS: 430\n",
      "global_step=192600, episodic_return=[0.]\n",
      "global_step=192900, episodic_return=[211.]\n",
      "global_step=193200, episodic_return=[0.]\n",
      "global_step=193500, episodic_return=[253.]\n",
      "global_step=193800, episodic_return=[0.]\n",
      "global_step=194100, episodic_return=[14.]\n",
      "global_step=194400, episodic_return=[226.]\n",
      "SPS: 429\n",
      "global_step=194700, episodic_return=[197.]\n",
      "global_step=195000, episodic_return=[0.]\n",
      "global_step=195300, episodic_return=[216.]\n",
      "global_step=195600, episodic_return=[0.]\n",
      "global_step=195900, episodic_return=[0.]\n",
      "global_step=196200, episodic_return=[225.]\n",
      "global_step=196500, episodic_return=[255.]\n",
      "SPS: 428\n",
      "global_step=196800, episodic_return=[231.]\n",
      "global_step=197100, episodic_return=[227.]\n",
      "global_step=197400, episodic_return=[233.]\n",
      "global_step=197700, episodic_return=[0.]\n",
      "global_step=198000, episodic_return=[241.]\n",
      "global_step=198300, episodic_return=[235.]\n",
      "global_step=198600, episodic_return=[204.]\n",
      "SPS: 427\n",
      "global_step=198900, episodic_return=[213.]\n",
      "global_step=199200, episodic_return=[50.]\n",
      "global_step=199500, episodic_return=[274.]\n",
      "global_step=199800, episodic_return=[273.]\n",
      "global_step=200100, episodic_return=[276.]\n",
      "global_step=200400, episodic_return=[228.]\n",
      "global_step=200700, episodic_return=[222.]\n",
      "SPS: 428\n",
      "global_step=201000, episodic_return=[0.]\n",
      "global_step=201300, episodic_return=[105.]\n",
      "global_step=201600, episodic_return=[206.]\n",
      "global_step=201900, episodic_return=[207.]\n",
      "global_step=202200, episodic_return=[0.]\n",
      "global_step=202500, episodic_return=[0.]\n",
      "SPS: 428\n",
      "global_step=202800, episodic_return=[238.]\n",
      "global_step=203100, episodic_return=[182.]\n",
      "global_step=203400, episodic_return=[0.]\n",
      "global_step=203700, episodic_return=[229.]\n",
      "global_step=204000, episodic_return=[166.]\n",
      "global_step=204300, episodic_return=[252.]\n",
      "global_step=204600, episodic_return=[230.]\n",
      "SPS: 428\n",
      "global_step=204900, episodic_return=[0.]\n",
      "global_step=205200, episodic_return=[249.]\n",
      "global_step=205500, episodic_return=[0.]\n",
      "global_step=205800, episodic_return=[228.]\n",
      "global_step=206100, episodic_return=[221.]\n",
      "global_step=206400, episodic_return=[105.]\n",
      "global_step=206700, episodic_return=[156.]\n",
      "SPS: 429\n",
      "global_step=207000, episodic_return=[73.]\n",
      "global_step=207300, episodic_return=[272.]\n",
      "global_step=207600, episodic_return=[0.]\n",
      "global_step=207900, episodic_return=[262.]\n",
      "global_step=208200, episodic_return=[180.]\n",
      "global_step=208500, episodic_return=[0.]\n",
      "global_step=208800, episodic_return=[0.]\n",
      "SPS: 429\n",
      "global_step=209100, episodic_return=[0.]\n",
      "global_step=209400, episodic_return=[0.]\n",
      "global_step=209700, episodic_return=[0.]\n",
      "global_step=210000, episodic_return=[249.]\n",
      "global_step=210300, episodic_return=[0.]\n",
      "global_step=210600, episodic_return=[0.]\n",
      "global_step=210900, episodic_return=[189.]\n",
      "SPS: 429\n",
      "global_step=211200, episodic_return=[0.]\n",
      "global_step=211500, episodic_return=[0.]\n",
      "global_step=211800, episodic_return=[0.]\n",
      "global_step=212100, episodic_return=[0.]\n",
      "global_step=212400, episodic_return=[0.]\n",
      "global_step=212700, episodic_return=[0.]\n",
      "SPS: 429\n",
      "global_step=213000, episodic_return=[0.]\n",
      "global_step=213300, episodic_return=[276.]\n",
      "global_step=213600, episodic_return=[243.]\n",
      "global_step=213900, episodic_return=[0.]\n",
      "global_step=214200, episodic_return=[0.]\n",
      "global_step=214500, episodic_return=[0.]\n",
      "global_step=214800, episodic_return=[0.]\n",
      "SPS: 429\n",
      "global_step=215100, episodic_return=[0.]\n",
      "global_step=215400, episodic_return=[147.]\n",
      "global_step=215700, episodic_return=[0.]\n",
      "global_step=216000, episodic_return=[252.]\n",
      "global_step=216300, episodic_return=[0.]\n",
      "global_step=216600, episodic_return=[0.]\n",
      "global_step=216900, episodic_return=[0.]\n",
      "SPS: 430\n",
      "global_step=217200, episodic_return=[80.]\n",
      "global_step=217500, episodic_return=[0.]\n",
      "global_step=217800, episodic_return=[0.]\n",
      "global_step=218100, episodic_return=[237.]\n",
      "global_step=218400, episodic_return=[0.]\n",
      "global_step=218700, episodic_return=[0.]\n",
      "global_step=219000, episodic_return=[139.]\n",
      "SPS: 430\n",
      "global_step=219300, episodic_return=[0.]\n",
      "global_step=219600, episodic_return=[186.]\n",
      "global_step=219900, episodic_return=[227.]\n",
      "global_step=220200, episodic_return=[0.]\n",
      "global_step=220500, episodic_return=[0.]\n",
      "global_step=220800, episodic_return=[263.]\n",
      "global_step=221100, episodic_return=[0.]\n",
      "SPS: 430\n",
      "global_step=221400, episodic_return=[241.]\n",
      "global_step=221700, episodic_return=[175.]\n",
      "global_step=222000, episodic_return=[213.]\n",
      "global_step=222300, episodic_return=[0.]\n",
      "global_step=222600, episodic_return=[274.]\n",
      "global_step=222900, episodic_return=[261.]\n",
      "global_step=223200, episodic_return=[218.]\n",
      "SPS: 430\n",
      "global_step=223500, episodic_return=[210.]\n",
      "global_step=223800, episodic_return=[253.]\n",
      "global_step=224100, episodic_return=[0.]\n",
      "global_step=224400, episodic_return=[0.]\n",
      "global_step=224700, episodic_return=[260.]\n",
      "global_step=225000, episodic_return=[193.]\n",
      "SPS: 430\n",
      "global_step=225300, episodic_return=[0.]\n",
      "global_step=225600, episodic_return=[0.]\n",
      "global_step=225900, episodic_return=[206.]\n",
      "global_step=226200, episodic_return=[279.]\n",
      "global_step=226500, episodic_return=[29.]\n",
      "global_step=226800, episodic_return=[0.]\n",
      "global_step=227100, episodic_return=[0.]\n",
      "SPS: 430\n",
      "global_step=227400, episodic_return=[0.]\n",
      "global_step=227700, episodic_return=[0.]\n",
      "global_step=228000, episodic_return=[0.]\n",
      "global_step=228300, episodic_return=[209.]\n",
      "global_step=228600, episodic_return=[0.]\n",
      "global_step=228900, episodic_return=[283.]\n",
      "global_step=229200, episodic_return=[168.]\n",
      "SPS: 430\n",
      "global_step=229500, episodic_return=[0.]\n",
      "global_step=229800, episodic_return=[0.]\n",
      "global_step=230100, episodic_return=[156.]\n",
      "global_step=230400, episodic_return=[0.]\n",
      "global_step=230700, episodic_return=[121.]\n",
      "global_step=231000, episodic_return=[0.]\n",
      "global_step=231300, episodic_return=[147.]\n",
      "SPS: 431\n",
      "global_step=231600, episodic_return=[0.]\n",
      "global_step=231900, episodic_return=[88.]\n",
      "global_step=232200, episodic_return=[211.]\n",
      "global_step=232500, episodic_return=[206.]\n",
      "global_step=232800, episodic_return=[0.]\n",
      "global_step=233100, episodic_return=[282.]\n",
      "global_step=233400, episodic_return=[273.]\n",
      "SPS: 431\n",
      "global_step=233700, episodic_return=[0.]\n",
      "global_step=234000, episodic_return=[62.]\n",
      "global_step=234300, episodic_return=[0.]\n",
      "global_step=234600, episodic_return=[0.]\n",
      "global_step=234900, episodic_return=[0.]\n",
      "global_step=235200, episodic_return=[203.]\n",
      "global_step=235500, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=235800, episodic_return=[274.]\n",
      "global_step=236100, episodic_return=[24.]\n",
      "global_step=236400, episodic_return=[175.]\n",
      "global_step=236700, episodic_return=[58.]\n",
      "global_step=237000, episodic_return=[70.]\n",
      "global_step=237300, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=237600, episodic_return=[226.]\n",
      "global_step=237900, episodic_return=[0.]\n",
      "global_step=238200, episodic_return=[217.]\n",
      "global_step=238500, episodic_return=[240.]\n",
      "global_step=238800, episodic_return=[0.]\n",
      "global_step=239100, episodic_return=[0.]\n",
      "global_step=239400, episodic_return=[10.]\n",
      "SPS: 431\n",
      "global_step=239700, episodic_return=[179.]\n",
      "global_step=240000, episodic_return=[0.]\n",
      "global_step=240300, episodic_return=[163.]\n",
      "global_step=240600, episodic_return=[0.]\n",
      "global_step=240900, episodic_return=[238.]\n",
      "global_step=241200, episodic_return=[0.]\n",
      "global_step=241500, episodic_return=[220.]\n",
      "SPS: 432\n",
      "global_step=241800, episodic_return=[261.]\n",
      "global_step=242100, episodic_return=[248.]\n",
      "global_step=242400, episodic_return=[0.]\n",
      "global_step=242700, episodic_return=[66.]\n",
      "global_step=243000, episodic_return=[269.]\n",
      "global_step=243300, episodic_return=[41.]\n",
      "global_step=243600, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=243900, episodic_return=[0.]\n",
      "global_step=244200, episodic_return=[0.]\n",
      "global_step=244500, episodic_return=[245.]\n",
      "global_step=244800, episodic_return=[0.]\n",
      "global_step=245100, episodic_return=[0.]\n",
      "global_step=245400, episodic_return=[184.]\n",
      "global_step=245700, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=246000, episodic_return=[200.]\n",
      "global_step=246300, episodic_return=[0.]\n",
      "global_step=246600, episodic_return=[0.]\n",
      "global_step=246900, episodic_return=[274.]\n",
      "global_step=247200, episodic_return=[226.]\n",
      "global_step=247500, episodic_return=[99.]\n",
      "global_step=247800, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=248100, episodic_return=[0.]\n",
      "global_step=248400, episodic_return=[0.]\n",
      "global_step=248700, episodic_return=[192.]\n",
      "global_step=249000, episodic_return=[40.]\n",
      "global_step=249300, episodic_return=[42.]\n",
      "global_step=249600, episodic_return=[84.]\n",
      "SPS: 432\n",
      "global_step=249900, episodic_return=[178.]\n",
      "global_step=250200, episodic_return=[0.]\n",
      "global_step=250500, episodic_return=[132.]\n",
      "global_step=250800, episodic_return=[130.]\n",
      "global_step=251100, episodic_return=[0.]\n",
      "global_step=251400, episodic_return=[0.]\n",
      "global_step=251700, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=252000, episodic_return=[0.]\n",
      "global_step=252300, episodic_return=[249.]\n",
      "global_step=252600, episodic_return=[265.]\n",
      "global_step=252900, episodic_return=[0.]\n",
      "global_step=253200, episodic_return=[5.]\n",
      "global_step=253500, episodic_return=[0.]\n",
      "global_step=253800, episodic_return=[247.]\n",
      "SPS: 432\n",
      "global_step=254100, episodic_return=[204.]\n",
      "global_step=254400, episodic_return=[0.]\n",
      "global_step=254700, episodic_return=[187.]\n",
      "global_step=255000, episodic_return=[0.]\n",
      "global_step=255300, episodic_return=[0.]\n",
      "global_step=255600, episodic_return=[0.]\n",
      "global_step=255900, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=256200, episodic_return=[0.]\n",
      "global_step=256500, episodic_return=[278.]\n",
      "global_step=256800, episodic_return=[0.]\n",
      "global_step=257100, episodic_return=[0.]\n",
      "global_step=257400, episodic_return=[0.]\n",
      "global_step=257700, episodic_return=[271.]\n",
      "global_step=258000, episodic_return=[274.]\n",
      "SPS: 433\n",
      "global_step=258300, episodic_return=[0.]\n",
      "global_step=258600, episodic_return=[0.]\n",
      "global_step=258900, episodic_return=[197.]\n",
      "global_step=259200, episodic_return=[0.]\n",
      "global_step=259500, episodic_return=[0.]\n",
      "global_step=259800, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=260100, episodic_return=[0.]\n",
      "global_step=260400, episodic_return=[213.]\n",
      "global_step=260700, episodic_return=[233.]\n",
      "global_step=261000, episodic_return=[0.]\n",
      "global_step=261300, episodic_return=[0.]\n",
      "global_step=261600, episodic_return=[194.]\n",
      "global_step=261900, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=262200, episodic_return=[162.]\n",
      "global_step=262500, episodic_return=[213.]\n",
      "global_step=262800, episodic_return=[196.]\n",
      "global_step=263100, episodic_return=[219.]\n",
      "global_step=263400, episodic_return=[265.]\n",
      "global_step=263700, episodic_return=[233.]\n",
      "global_step=264000, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=264300, episodic_return=[15.]\n",
      "global_step=264600, episodic_return=[204.]\n",
      "global_step=264900, episodic_return=[195.]\n",
      "global_step=265200, episodic_return=[184.]\n",
      "global_step=265500, episodic_return=[218.]\n",
      "global_step=265800, episodic_return=[0.]\n",
      "global_step=266100, episodic_return=[172.]\n",
      "SPS: 433\n",
      "global_step=266400, episodic_return=[226.]\n",
      "global_step=266700, episodic_return=[0.]\n",
      "global_step=267000, episodic_return=[148.]\n",
      "global_step=267300, episodic_return=[0.]\n",
      "global_step=267600, episodic_return=[272.]\n",
      "global_step=267900, episodic_return=[211.]\n",
      "global_step=268200, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=268500, episodic_return=[219.]\n",
      "global_step=268800, episodic_return=[0.]\n",
      "global_step=269100, episodic_return=[285.]\n",
      "global_step=269400, episodic_return=[0.]\n",
      "global_step=269700, episodic_return=[162.]\n",
      "global_step=270000, episodic_return=[0.]\n",
      "global_step=270300, episodic_return=[122.]\n",
      "SPS: 433\n",
      "global_step=270600, episodic_return=[0.]\n",
      "global_step=270900, episodic_return=[0.]\n",
      "global_step=271200, episodic_return=[0.]\n",
      "global_step=271500, episodic_return=[103.]\n",
      "global_step=271800, episodic_return=[0.]\n",
      "global_step=272100, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=272400, episodic_return=[0.]\n",
      "global_step=272700, episodic_return=[0.]\n",
      "global_step=273000, episodic_return=[230.]\n",
      "global_step=273300, episodic_return=[0.]\n",
      "global_step=273600, episodic_return=[13.]\n",
      "global_step=273900, episodic_return=[0.]\n",
      "global_step=274200, episodic_return=[105.]\n",
      "SPS: 434\n",
      "global_step=274500, episodic_return=[23.]\n",
      "global_step=274800, episodic_return=[0.]\n",
      "global_step=275100, episodic_return=[0.]\n",
      "global_step=275400, episodic_return=[217.]\n",
      "global_step=275700, episodic_return=[95.]\n",
      "global_step=276000, episodic_return=[0.]\n",
      "global_step=276300, episodic_return=[260.]\n",
      "SPS: 434\n",
      "global_step=276600, episodic_return=[253.]\n",
      "global_step=276900, episodic_return=[167.]\n",
      "global_step=277200, episodic_return=[258.]\n",
      "global_step=277500, episodic_return=[0.]\n",
      "global_step=277800, episodic_return=[0.]\n",
      "global_step=278100, episodic_return=[94.]\n",
      "global_step=278400, episodic_return=[167.]\n",
      "SPS: 434\n",
      "global_step=278700, episodic_return=[262.]\n",
      "global_step=279000, episodic_return=[153.]\n",
      "global_step=279300, episodic_return=[261.]\n",
      "global_step=279600, episodic_return=[278.]\n",
      "global_step=279900, episodic_return=[137.]\n",
      "global_step=280200, episodic_return=[0.]\n",
      "global_step=280500, episodic_return=[232.]\n",
      "SPS: 434\n",
      "global_step=280800, episodic_return=[245.]\n",
      "global_step=281100, episodic_return=[0.]\n",
      "global_step=281400, episodic_return=[0.]\n",
      "global_step=281700, episodic_return=[0.]\n",
      "global_step=282000, episodic_return=[0.]\n",
      "global_step=282300, episodic_return=[0.]\n",
      "global_step=282600, episodic_return=[171.]\n",
      "SPS: 434\n",
      "global_step=282900, episodic_return=[192.]\n",
      "global_step=283200, episodic_return=[72.]\n",
      "global_step=283500, episodic_return=[0.]\n",
      "global_step=283800, episodic_return=[0.]\n",
      "global_step=284100, episodic_return=[0.]\n",
      "global_step=284400, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=284700, episodic_return=[160.]\n",
      "global_step=285000, episodic_return=[0.]\n",
      "global_step=285300, episodic_return=[140.]\n",
      "global_step=285600, episodic_return=[0.]\n",
      "global_step=285900, episodic_return=[0.]\n",
      "global_step=286200, episodic_return=[0.]\n",
      "global_step=286500, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=286800, episodic_return=[268.]\n",
      "global_step=287100, episodic_return=[73.]\n",
      "global_step=287400, episodic_return=[258.]\n",
      "global_step=287700, episodic_return=[150.]\n",
      "global_step=288000, episodic_return=[242.]\n",
      "global_step=288300, episodic_return=[0.]\n",
      "global_step=288600, episodic_return=[30.]\n",
      "SPS: 434\n",
      "global_step=288900, episodic_return=[0.]\n",
      "global_step=289200, episodic_return=[227.]\n",
      "global_step=289500, episodic_return=[0.]\n",
      "global_step=289800, episodic_return=[0.]\n",
      "global_step=290100, episodic_return=[130.]\n",
      "global_step=290400, episodic_return=[19.]\n",
      "global_step=290700, episodic_return=[287.]\n",
      "SPS: 434\n",
      "global_step=291000, episodic_return=[0.]\n",
      "global_step=291300, episodic_return=[248.]\n",
      "global_step=291600, episodic_return=[188.]\n",
      "global_step=291900, episodic_return=[0.]\n",
      "global_step=292200, episodic_return=[280.]\n",
      "global_step=292500, episodic_return=[0.]\n",
      "global_step=292800, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=293100, episodic_return=[0.]\n",
      "global_step=293400, episodic_return=[149.]\n",
      "global_step=293700, episodic_return=[0.]\n",
      "global_step=294000, episodic_return=[199.]\n",
      "global_step=294300, episodic_return=[0.]\n",
      "global_step=294600, episodic_return=[0.]\n",
      "global_step=294900, episodic_return=[0.]\n",
      "SPS: 434\n",
      "global_step=295200, episodic_return=[267.]\n",
      "global_step=295500, episodic_return=[0.]\n",
      "global_step=295800, episodic_return=[223.]\n",
      "global_step=296100, episodic_return=[19.]\n",
      "global_step=296400, episodic_return=[205.]\n",
      "global_step=296700, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=297000, episodic_return=[0.]\n",
      "global_step=297300, episodic_return=[0.]\n",
      "global_step=297600, episodic_return=[236.]\n",
      "global_step=297900, episodic_return=[59.]\n",
      "global_step=298200, episodic_return=[0.]\n",
      "global_step=298500, episodic_return=[0.]\n",
      "global_step=298800, episodic_return=[133.]\n",
      "SPS: 435\n",
      "global_step=299100, episodic_return=[38.]\n",
      "global_step=299400, episodic_return=[284.]\n",
      "global_step=299700, episodic_return=[220.]\n",
      "global_step=300000, episodic_return=[0.]\n",
      "global_step=300300, episodic_return=[175.]\n",
      "global_step=300600, episodic_return=[123.]\n",
      "global_step=300900, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=301200, episodic_return=[0.]\n",
      "global_step=301500, episodic_return=[0.]\n",
      "global_step=301800, episodic_return=[0.]\n",
      "global_step=302100, episodic_return=[0.]\n",
      "global_step=302400, episodic_return=[0.]\n",
      "global_step=302700, episodic_return=[101.]\n",
      "global_step=303000, episodic_return=[124.]\n",
      "SPS: 435\n",
      "global_step=303300, episodic_return=[262.]\n",
      "global_step=303600, episodic_return=[0.]\n",
      "global_step=303900, episodic_return=[43.]\n",
      "global_step=304200, episodic_return=[218.]\n",
      "global_step=304500, episodic_return=[187.]\n",
      "global_step=304800, episodic_return=[55.]\n",
      "global_step=305100, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=305400, episodic_return=[276.]\n",
      "global_step=305700, episodic_return=[0.]\n",
      "global_step=306000, episodic_return=[178.]\n",
      "global_step=306300, episodic_return=[0.]\n",
      "global_step=306600, episodic_return=[250.]\n",
      "global_step=306900, episodic_return=[115.]\n",
      "global_step=307200, episodic_return=[266.]\n",
      "SPS: 435\n",
      "global_step=307500, episodic_return=[0.]\n",
      "global_step=307800, episodic_return=[229.]\n",
      "global_step=308100, episodic_return=[252.]\n",
      "global_step=308400, episodic_return=[218.]\n",
      "global_step=308700, episodic_return=[0.]\n",
      "global_step=309000, episodic_return=[240.]\n",
      "SPS: 435\n",
      "global_step=309300, episodic_return=[28.]\n",
      "global_step=309600, episodic_return=[176.]\n",
      "global_step=309900, episodic_return=[0.]\n",
      "global_step=310200, episodic_return=[270.]\n",
      "global_step=310500, episodic_return=[172.]\n",
      "global_step=310800, episodic_return=[166.]\n",
      "global_step=311100, episodic_return=[280.]\n",
      "SPS: 436\n",
      "global_step=311400, episodic_return=[238.]\n",
      "global_step=311700, episodic_return=[0.]\n",
      "global_step=312000, episodic_return=[0.]\n",
      "global_step=312300, episodic_return=[226.]\n",
      "global_step=312600, episodic_return=[260.]\n",
      "global_step=312900, episodic_return=[273.]\n",
      "global_step=313200, episodic_return=[267.]\n",
      "SPS: 436\n",
      "global_step=313500, episodic_return=[250.]\n",
      "global_step=313800, episodic_return=[0.]\n",
      "global_step=314100, episodic_return=[275.]\n",
      "global_step=314400, episodic_return=[0.]\n",
      "global_step=314700, episodic_return=[189.]\n",
      "global_step=315000, episodic_return=[206.]\n",
      "global_step=315300, episodic_return=[235.]\n",
      "SPS: 436\n",
      "global_step=315600, episodic_return=[0.]\n",
      "global_step=315900, episodic_return=[220.]\n",
      "global_step=316200, episodic_return=[252.]\n",
      "global_step=316500, episodic_return=[107.]\n",
      "global_step=316800, episodic_return=[258.]\n",
      "global_step=317100, episodic_return=[134.]\n",
      "global_step=317400, episodic_return=[230.]\n",
      "SPS: 436\n",
      "global_step=317700, episodic_return=[0.]\n",
      "global_step=318000, episodic_return=[179.]\n",
      "global_step=318300, episodic_return=[212.]\n",
      "global_step=318600, episodic_return=[245.]\n",
      "global_step=318900, episodic_return=[0.]\n",
      "global_step=319200, episodic_return=[254.]\n",
      "SPS: 436\n",
      "global_step=319500, episodic_return=[0.]\n",
      "global_step=319800, episodic_return=[0.]\n",
      "global_step=320100, episodic_return=[0.]\n",
      "global_step=320400, episodic_return=[202.]\n",
      "global_step=320700, episodic_return=[125.]\n",
      "global_step=321000, episodic_return=[0.]\n",
      "global_step=321300, episodic_return=[248.]\n",
      "SPS: 435\n",
      "global_step=321600, episodic_return=[0.]\n",
      "global_step=321900, episodic_return=[216.]\n",
      "global_step=322200, episodic_return=[189.]\n",
      "global_step=322500, episodic_return=[0.]\n",
      "global_step=322800, episodic_return=[0.]\n",
      "global_step=323100, episodic_return=[129.]\n",
      "global_step=323400, episodic_return=[246.]\n",
      "SPS: 435\n",
      "global_step=323700, episodic_return=[199.]\n",
      "global_step=324000, episodic_return=[0.]\n",
      "global_step=324300, episodic_return=[0.]\n",
      "global_step=324600, episodic_return=[241.]\n",
      "global_step=324900, episodic_return=[44.]\n",
      "global_step=325200, episodic_return=[0.]\n",
      "global_step=325500, episodic_return=[0.]\n",
      "SPS: 435\n",
      "global_step=325800, episodic_return=[0.]\n",
      "global_step=326100, episodic_return=[211.]\n",
      "global_step=326400, episodic_return=[247.]\n",
      "global_step=326700, episodic_return=[0.]\n",
      "global_step=327000, episodic_return=[259.]\n",
      "global_step=327300, episodic_return=[275.]\n",
      "global_step=327600, episodic_return=[242.]\n",
      "SPS: 434\n",
      "global_step=327900, episodic_return=[212.]\n",
      "global_step=328200, episodic_return=[195.]\n",
      "global_step=328500, episodic_return=[269.]\n",
      "global_step=328800, episodic_return=[225.]\n",
      "global_step=329100, episodic_return=[221.]\n",
      "global_step=329400, episodic_return=[0.]\n",
      "global_step=329700, episodic_return=[233.]\n",
      "SPS: 434\n",
      "global_step=330000, episodic_return=[89.]\n",
      "global_step=330300, episodic_return=[227.]\n",
      "global_step=330600, episodic_return=[202.]\n",
      "global_step=330900, episodic_return=[0.]\n",
      "global_step=331200, episodic_return=[0.]\n",
      "global_step=331500, episodic_return=[0.]\n",
      "SPS: 433\n",
      "global_step=331800, episodic_return=[207.]\n",
      "global_step=332100, episodic_return=[44.]\n",
      "global_step=332400, episodic_return=[0.]\n",
      "global_step=332700, episodic_return=[274.]\n",
      "global_step=333000, episodic_return=[219.]\n",
      "global_step=333300, episodic_return=[262.]\n",
      "global_step=333600, episodic_return=[238.]\n",
      "SPS: 433\n",
      "global_step=333900, episodic_return=[251.]\n",
      "global_step=334200, episodic_return=[0.]\n",
      "global_step=334500, episodic_return=[134.]\n",
      "global_step=334800, episodic_return=[205.]\n",
      "global_step=335100, episodic_return=[0.]\n",
      "global_step=335400, episodic_return=[6.]\n",
      "global_step=335700, episodic_return=[274.]\n",
      "SPS: 432\n",
      "global_step=336000, episodic_return=[241.]\n",
      "global_step=336300, episodic_return=[224.]\n",
      "global_step=336600, episodic_return=[0.]\n",
      "global_step=336900, episodic_return=[278.]\n",
      "global_step=337200, episodic_return=[188.]\n",
      "global_step=337500, episodic_return=[0.]\n",
      "global_step=337800, episodic_return=[0.]\n",
      "SPS: 432\n",
      "global_step=338100, episodic_return=[246.]\n",
      "global_step=338400, episodic_return=[0.]\n",
      "global_step=338700, episodic_return=[246.]\n",
      "global_step=339000, episodic_return=[238.]\n",
      "global_step=339300, episodic_return=[0.]\n",
      "global_step=339600, episodic_return=[0.]\n",
      "global_step=339900, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=340200, episodic_return=[242.]\n",
      "global_step=340500, episodic_return=[232.]\n",
      "global_step=340800, episodic_return=[278.]\n",
      "global_step=341100, episodic_return=[0.]\n",
      "global_step=341400, episodic_return=[206.]\n",
      "global_step=341700, episodic_return=[202.]\n",
      "global_step=342000, episodic_return=[227.]\n",
      "SPS: 431\n",
      "global_step=342300, episodic_return=[204.]\n",
      "global_step=342600, episodic_return=[256.]\n",
      "global_step=342900, episodic_return=[30.]\n",
      "global_step=343200, episodic_return=[253.]\n",
      "global_step=343500, episodic_return=[0.]\n",
      "global_step=343800, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=344100, episodic_return=[185.]\n",
      "global_step=344400, episodic_return=[0.]\n",
      "global_step=344700, episodic_return=[0.]\n",
      "global_step=345000, episodic_return=[278.]\n",
      "global_step=345300, episodic_return=[0.]\n",
      "global_step=345600, episodic_return=[254.]\n",
      "global_step=345900, episodic_return=[188.]\n",
      "SPS: 431\n",
      "global_step=346200, episodic_return=[123.]\n",
      "global_step=346500, episodic_return=[0.]\n",
      "global_step=346800, episodic_return=[0.]\n",
      "global_step=347100, episodic_return=[0.]\n",
      "global_step=347400, episodic_return=[0.]\n",
      "global_step=347700, episodic_return=[188.]\n",
      "global_step=348000, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=348300, episodic_return=[139.]\n",
      "global_step=348600, episodic_return=[201.]\n",
      "global_step=348900, episodic_return=[117.]\n",
      "global_step=349200, episodic_return=[259.]\n",
      "global_step=349500, episodic_return=[0.]\n",
      "global_step=349800, episodic_return=[200.]\n",
      "global_step=350100, episodic_return=[232.]\n",
      "SPS: 431\n",
      "global_step=350400, episodic_return=[228.]\n",
      "global_step=350700, episodic_return=[178.]\n",
      "global_step=351000, episodic_return=[10.]\n",
      "global_step=351300, episodic_return=[0.]\n",
      "global_step=351600, episodic_return=[0.]\n",
      "global_step=351900, episodic_return=[0.]\n",
      "global_step=352200, episodic_return=[201.]\n",
      "SPS: 431\n",
      "global_step=352500, episodic_return=[266.]\n",
      "global_step=352800, episodic_return=[66.]\n",
      "global_step=353100, episodic_return=[0.]\n",
      "global_step=353400, episodic_return=[231.]\n",
      "global_step=353700, episodic_return=[0.]\n",
      "global_step=354000, episodic_return=[0.]\n",
      "global_step=354300, episodic_return=[156.]\n",
      "SPS: 431\n",
      "global_step=354600, episodic_return=[164.]\n",
      "global_step=354900, episodic_return=[0.]\n",
      "global_step=355200, episodic_return=[206.]\n",
      "global_step=355500, episodic_return=[217.]\n",
      "global_step=355800, episodic_return=[180.]\n",
      "global_step=356100, episodic_return=[253.]\n",
      "SPS: 431\n",
      "global_step=356400, episodic_return=[231.]\n",
      "global_step=356700, episodic_return=[229.]\n",
      "global_step=357000, episodic_return=[0.]\n",
      "global_step=357300, episodic_return=[206.]\n",
      "global_step=357600, episodic_return=[0.]\n",
      "global_step=357900, episodic_return=[0.]\n",
      "global_step=358200, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=358500, episodic_return=[0.]\n",
      "global_step=358800, episodic_return=[0.]\n",
      "global_step=359100, episodic_return=[244.]\n",
      "global_step=359400, episodic_return=[93.]\n",
      "global_step=359700, episodic_return=[276.]\n",
      "global_step=360000, episodic_return=[207.]\n",
      "global_step=360300, episodic_return=[0.]\n",
      "SPS: 431\n",
      "global_step=360600, episodic_return=[0.]\n",
      "global_step=360900, episodic_return=[0.]\n",
      "global_step=361200, episodic_return=[207.]\n",
      "global_step=361500, episodic_return=[204.]\n",
      "global_step=361800, episodic_return=[0.]\n",
      "global_step=362100, episodic_return=[0.]\n",
      "global_step=362400, episodic_return=[230.]\n",
      "SPS: 431\n",
      "global_step=362700, episodic_return=[203.]\n",
      "global_step=363000, episodic_return=[155.]\n",
      "global_step=363300, episodic_return=[169.]\n",
      "global_step=363600, episodic_return=[212.]\n",
      "global_step=363900, episodic_return=[171.]\n",
      "global_step=364200, episodic_return=[0.]\n",
      "global_step=364500, episodic_return=[262.]\n",
      "SPS: 430\n",
      "global_step=364800, episodic_return=[0.]\n",
      "global_step=365100, episodic_return=[276.]\n",
      "global_step=365400, episodic_return=[261.]\n",
      "global_step=365700, episodic_return=[262.]\n",
      "global_step=366000, episodic_return=[283.]\n",
      "global_step=366300, episodic_return=[0.]\n",
      "SPS: 430\n",
      "global_step=366600, episodic_return=[0.]\n",
      "global_step=366900, episodic_return=[0.]\n",
      "global_step=367200, episodic_return=[0.]\n",
      "global_step=367500, episodic_return=[143.]\n",
      "global_step=367800, episodic_return=[0.]\n",
      "global_step=368100, episodic_return=[0.]\n",
      "global_step=368400, episodic_return=[0.]\n",
      "SPS: 429\n",
      "global_step=368700, episodic_return=[226.]\n",
      "global_step=369000, episodic_return=[232.]\n",
      "global_step=369300, episodic_return=[242.]\n",
      "global_step=369600, episodic_return=[40.]\n",
      "global_step=369900, episodic_return=[0.]\n",
      "global_step=370200, episodic_return=[134.]\n",
      "global_step=370500, episodic_return=[0.]\n",
      "SPS: 429\n",
      "global_step=370800, episodic_return=[0.]\n",
      "global_step=371100, episodic_return=[0.]\n",
      "global_step=371400, episodic_return=[4.]\n",
      "global_step=371700, episodic_return=[245.]\n",
      "global_step=372000, episodic_return=[225.]\n",
      "global_step=372300, episodic_return=[230.]\n",
      "global_step=372600, episodic_return=[130.]\n",
      "SPS: 428\n",
      "global_step=372900, episodic_return=[0.]\n",
      "global_step=373200, episodic_return=[263.]\n",
      "global_step=373500, episodic_return=[47.]\n",
      "global_step=373800, episodic_return=[0.]\n",
      "global_step=374100, episodic_return=[0.]\n",
      "global_step=374400, episodic_return=[0.]\n",
      "global_step=374700, episodic_return=[3.]\n",
      "SPS: 428\n",
      "global_step=375000, episodic_return=[0.]\n",
      "global_step=375300, episodic_return=[221.]\n",
      "global_step=375600, episodic_return=[201.]\n",
      "global_step=375900, episodic_return=[148.]\n",
      "global_step=376200, episodic_return=[0.]\n",
      "global_step=376500, episodic_return=[196.]\n",
      "global_step=376800, episodic_return=[0.]\n",
      "SPS: 427\n",
      "global_step=377100, episodic_return=[240.]\n",
      "global_step=377400, episodic_return=[0.]\n",
      "global_step=377700, episodic_return=[245.]\n",
      "global_step=378000, episodic_return=[0.]\n",
      "global_step=378300, episodic_return=[251.]\n",
      "global_step=378600, episodic_return=[0.]\n",
      "SPS: 428\n",
      "global_step=378900, episodic_return=[239.]\n",
      "global_step=379200, episodic_return=[164.]\n",
      "global_step=379500, episodic_return=[214.]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, args.num_iterations + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if args.anneal_lr:\n",
    "        frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "        lrnow = frac * args.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += args.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "if args.save_model:\n",
    "    model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "    torch.save(agent.state_dict(), model_path)\n",
    "    print(f\"model saved to {model_path}\")\n",
    "    from cleanrl_utils.evals.ppo_eval import evaluate\n",
    "\n",
    "    episodic_returns = evaluate(\n",
    "        model_path,\n",
    "        make_env,\n",
    "        args.env_id,\n",
    "        eval_episodes=10,\n",
    "        run_name=f\"{run_name}-eval\",\n",
    "        Model=Agent,\n",
    "        device=device,\n",
    "        gamma=args.gamma,\n",
    "    )\n",
    "    for idx, episodic_return in enumerate(episodic_returns):\n",
    "        writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "    if args.upload_model:\n",
    "        from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "        repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "        repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "        push_to_hub(args, episodic_returns, repo_id, \"PPO\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
