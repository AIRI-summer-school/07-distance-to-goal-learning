{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d57d13",
   "metadata": {},
   "source": [
    "# **Stage 1**: Train PPO on PointMaze with standard rewards, collect data, train distance models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18526311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.env_wrappers import EnvBuilder\n",
    "from src import ppo_agent, distance_models\n",
    "from src.maze_visuzlization import demo_pointmaze\n",
    "\n",
    "# Check for GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path \n",
    "agent = ppo_agent.PPOAgent(\n",
    "    state_dim=self.observation_space.shape[0],\n",
    "    action_dim=self.action_space.shape[0]\n",
    ")\n",
    "agent.ac.load_state_dict(torch.load(model_path, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36073d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10/48 [ 20%] | AvgReturn: 0.00 | ETA: 0.9 min\n",
      "Update 20/48 [ 41%] | AvgReturn: 0.00 | ETA: 0.8 min\n",
      "Update 30/48 [ 62%] | AvgReturn: 0.00 | ETA: 0.5 min\n",
      "Update 40/48 [ 83%] | AvgReturn: 0.00 | ETA: 0.2 min\n"
     ]
    }
   ],
   "source": [
    "# Initialize PPO agent\n",
    "agent = ppo_agent.PPOAgent(state_dim=obs_dim, action_dim=act_dim)\n",
    "writer = SummaryWriter(log_dir=\"runs/stage1\")\n",
    "global_step = 0\n",
    "num_updates = total_timesteps // steps_per_iter\n",
    "for update in range(1, num_updates+1):\n",
    "    traj = agent.collect_trajectory(env, steps_per_iter)\n",
    "    pg_loss, v_loss, ent_loss = agent.update(traj)\n",
    "    # Log training metrics\n",
    "    rewards = traj[\"rewards\"]; dones = traj[\"dones\"]\n",
    "    ep_returns = []\n",
    "    cum_reward = 0.0\n",
    "    for r, d in zip(rewards, dones):\n",
    "        cum_reward += r\n",
    "        if d:\n",
    "            ep_returns.append(cum_reward)\n",
    "            cum_reward = 0.0\n",
    "    if ep_returns:\n",
    "        writer.add_scalar(\"charts/episodic_return\", np.mean(ep_returns), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss, global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss, global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", ent_loss, global_step)\n",
    "    global_step += len(rewards)\n",
    "\n",
    "    if update == 1: start_time = time.time()\n",
    "    if update % 10 == 0:\n",
    "        avg_ret = np.mean(ep_returns) if ep_returns else 0.0\n",
    "        elapsed = time.time() - start_time\n",
    "        updates_done = update\n",
    "        updates_left = num_updates - updates_done\n",
    "        time_per_update = elapsed / updates_done\n",
    "        eta = updates_left * time_per_update\n",
    "        print(f\"Update {update}/{num_updates} [{int(100*update/num_updates):3d}%] | AvgReturn: {avg_ret:.2f} | ETA: {eta/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a167348",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unexpected mode: None, expected modes: human, rgb_array, depth_array, or rgbd_tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdemonstrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/ppo_agent_stage1.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/projects/RL_AIRI/artem_ws/src/env_wrappers.py:91\u001b[39m, in \u001b[36mDemonstrateWrapper.demonstrate\u001b[39m\u001b[34m(self, model_path, out_dir, filename, fps, deterministic)\u001b[39m\n\u001b[32m     89\u001b[39m     act = agent.ac.act(obs, deterministic=deterministic)[\u001b[32m0\u001b[39m]\n\u001b[32m     90\u001b[39m obs, _, term, trunc, _ = \u001b[38;5;28mself\u001b[39m.step(act)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m frame = np.ascontiguousarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     93\u001b[39m dist = np.hypot(obs[\u001b[32m0\u001b[39m] - obs[\u001b[32m4\u001b[39m], obs[\u001b[32m1\u001b[39m] - obs[\u001b[32m5\u001b[39m])\n\u001b[32m     94\u001b[39m draw_action_arrow(frame, act)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Wrapper.render at line 337 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/wrappers/common.py:301\u001b[39m, in \u001b[36mPassiveEnvChecker.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_render = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.render()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:361\u001b[39m, in \u001b[36menv_render_passive_checker\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    356\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env.render_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[32m    357\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv.render_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     _check_render_return(env.render_mode, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium_robotics/envs/maze/point_maze.py:422\u001b[39m, in \u001b[36mPointMazeEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpoint_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py:158\u001b[39m, in \u001b[36mMujocoEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    155\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    Render a frame from the MuJoCo simulation as specified by the render_mode.\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmujoco_renderer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:761\u001b[39m, in \u001b[36mMujocoRenderer.render\u001b[39m\u001b[34m(self, render_mode)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m render_mode != \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    758\u001b[39m         \u001b[38;5;28mself\u001b[39m.width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.height \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    759\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe width: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.width\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and height: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.height\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be `None` when the render_mode is not `human`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m viewer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_viewer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m render_mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdepth_array\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrgbd_tuple\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m viewer.render(render_mode=render_mode, camera_id=\u001b[38;5;28mself\u001b[39m.camera_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/libs/miniforge3/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:794\u001b[39m, in \u001b[36mMujocoRenderer._get_viewer\u001b[39m\u001b[34m(self, render_mode)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer = OffScreenViewer(\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    787\u001b[39m         \u001b[38;5;28mself\u001b[39m.data,\n\u001b[32m   (...)\u001b[39m\u001b[32m    791\u001b[39m         \u001b[38;5;28mself\u001b[39m._vopt,\n\u001b[32m    792\u001b[39m     )\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    795\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, expected modes: human, rgb_array, depth_array, or rgbd_tuple\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m     )\n\u001b[32m    797\u001b[39m \u001b[38;5;66;03m# Add default camera parameters\u001b[39;00m\n\u001b[32m    798\u001b[39m \u001b[38;5;28mself\u001b[39m._set_cam_config()\n",
      "\u001b[31mAttributeError\u001b[39m: Unexpected mode: None, expected modes: human, rgb_array, depth_array, or rgbd_tuple"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200b5871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10/48 [ 20%] | AvgReturn: 1.00 | ETA: 1.1 min\n",
      "Update 20/48 [ 41%] | AvgReturn: 1.00 | ETA: 0.8 min\n",
      "Update 30/48 [ 62%] | AvgReturn: 0.98 | ETA: 0.5 min\n",
      "Update 40/48 [ 83%] | AvgReturn: 0.88 | ETA: 0.2 min\n"
     ]
    }
   ],
   "source": [
    "# Initialize PPO agent\n",
    "agent = ppo_agent.PPOAgent(state_dim=obs_dim, action_dim=act_dim)\n",
    "writer = SummaryWriter(log_dir=\"runs/stage1\")\n",
    "global_step = 0\n",
    "num_updates = total_timesteps // steps_per_iter\n",
    "for update in range(1, num_updates+1):\n",
    "    traj = agent.collect_trajectory(env, steps_per_iter)\n",
    "    pg_loss, v_loss, ent_loss = agent.update(traj)\n",
    "    # Log training metrics\n",
    "    rewards = traj[\"rewards\"]; dones = traj[\"dones\"]\n",
    "    ep_returns = []\n",
    "    cum_reward = 0.0\n",
    "    for r, d in zip(rewards, dones):\n",
    "        cum_reward += r\n",
    "        if d:\n",
    "            ep_returns.append(cum_reward)\n",
    "            cum_reward = 0.0\n",
    "    if ep_returns:\n",
    "        writer.add_scalar(\"charts/episodic_return\", np.mean(ep_returns), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss, global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss, global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", ent_loss, global_step)\n",
    "    global_step += len(rewards)\n",
    "\n",
    "    if update == 1: start_time = time.time()\n",
    "    if update % 10 == 0:\n",
    "        avg_ret = np.mean(ep_returns) if ep_returns else 0.0\n",
    "        elapsed = time.time() - start_time\n",
    "        updates_done = update\n",
    "        updates_left = num_updates - updates_done\n",
    "        time_per_update = elapsed / updates_done\n",
    "        eta = updates_left * time_per_update\n",
    "        print(f\"Update {update}/{num_updates} [{int(100*update/num_updates):3d}%] | AvgReturn: {avg_ret:.2f} | ETA: {eta/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a481d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training PPO, collect trajectories to train distance models\n",
    "eval_episodes = 100\n",
    "sup_states = []\n",
    "sup_distances = []      \n",
    "td_transitions = []\n",
    "success_count = 0\n",
    "for ep in range(eval_episodes):\n",
    "    state, _ = eval_env.reset()\n",
    "    ep_states = [state]\n",
    "    transitions = []\n",
    "    cum_reward = 0.0\n",
    "    step_count = 0\n",
    "    success = False\n",
    "    while True:\n",
    "        action, logp, val = agent.ac.act(state)  # use trained policy\n",
    "        next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        transitions.append((state, next_state, done, bool(info.get('success', False))))\n",
    "        cum_reward += reward\n",
    "        step_count += 1\n",
    "        state = next_state\n",
    "        ep_states.append(state)\n",
    "        if done:\n",
    "            success = info.get('success', False)\n",
    "            if success:\n",
    "                success_count += 1\n",
    "                # For each state in this successful episode, record true distance to goal\n",
    "                # If episode length = step_count, distance for state[i] = step_count - i\n",
    "                for i in range(step_count):\n",
    "                    sup_states.append(ep_states[i])\n",
    "                    sup_distances.append(step_count - i)\n",
    "                # Include the final goal state with distance 0\n",
    "                sup_states.append(ep_states[-1])\n",
    "                sup_distances.append(0.0)\n",
    "            # Add all transitions to TD dataset (failures will be handled in training)\n",
    "            td_transitions.extend(transitions)\n",
    "            break\n",
    "print(f\"Collected data from {eval_episodes} episodes, {success_count} were successful.\")\n",
    "torch.save(agent.ac.state_dict(), \"models/ppo_agent_stage1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638550e",
   "metadata": {},
   "source": [
    "#### Check train result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pointmaze(model_path=\"models/ppo_agent_stage1.pth\", env = builder(seed=1, only_render = True), fps=20, deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671e295",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Generate dataset\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_states = np.array(sup_states, dtype=np.float32)\n",
    "sup_distances = np.array(sup_distances, dtype=np.float32)\n",
    "\n",
    "# Train distance estimators on the collected data\n",
    "sup_model = distance_models.SupervisedDistanceEstimator(input_dim=obs_dim)\n",
    "sup_loss = sup_model.train_from_data(sup_states, sup_distances, epochs=100)\n",
    "# td_model = distance_models.TDDistanceEstimator(input_dim=obs_dim)\n",
    "# td_loss = td_model.train_from_transitions(td_transitions, epochs=100)\n",
    "# Compare models on the supervised dataset\n",
    "sup_preds = sup_model.model(torch.tensor(sup_states)).detach().numpy().flatten()\n",
    "# td_preds = td_model.model(torch.tensor(sup_states)).detach().numpy().flatten()\n",
    "mse_sup = np.mean((sup_preds - sup_distances)**2)\n",
    "# mse_td = np.mean((td_preds - sup_distances)**2)\n",
    "\n",
    "print(f\"Supervised model MSE on training data: {mse_sup:.4f}\")\n",
    "# print(f\"TD model MSE on training data: {mse_td:.4f}\")\n",
    "\n",
    "# Save models for Stage 2\n",
    "# torch.save(td_model.state_dict(), \"models/distance_model_td.pth\")\n",
    "torch.save(sup_model.state_dict(), \"models/distance_model_sup.pth\")\n",
    "torch.save(agent.ac.state_dict(), \"models/ppo_agent_stage1.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc70f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
