{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d57d13",
   "metadata": {},
   "source": [
    "# **Stage 1**: Train PPO on PointMaze with standard rewards, collect data, train distance models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18526311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.env_wrappers import GoalObservationWrapper, TerminateOnSuccessWrapper\n",
    "from src import ppo_agent, distance_models\n",
    "\n",
    "# Check for GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a2fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy U-shaped maze\n",
    "# Letter 'c' means start and reward place. Each iteration is random\n",
    "c = 'c'\n",
    "example_map = [\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, c, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1],\n",
    "    [1, c, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc78329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'PointMaze_UMaze-v3'  # Use dense reward for initial training\n",
    "total_timesteps = 50000\n",
    "steps_per_iter = 1000\n",
    "seed = 0\n",
    "torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "# Initialize environment\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "env = gym.make(env_id, maze_map=example_map)\n",
    "env = GoalObservationWrapper(env)\n",
    "env = TerminateOnSuccessWrapper(env)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "200b5871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10/50 [ 20%] | AvgReturn: 0.00 | ETA: 0.7 min\n",
      "Update 20/50 [ 40%] | AvgReturn: 0.00 | ETA: 0.6 min\n",
      "Update 30/50 [ 60%] | AvgReturn: 0.00 | ETA: 0.4 min\n",
      "Update 40/50 [ 80%] | AvgReturn: 0.00 | ETA: 0.2 min\n",
      "Update 50/50 [100%] | AvgReturn: 0.00 | ETA: 0.0 min\n"
     ]
    }
   ],
   "source": [
    "# Initialize PPO agent\n",
    "agent = ppo_agent.PPOAgent(state_dim=obs_dim, action_dim=act_dim)\n",
    "writer = SummaryWriter(log_dir=\"runs/stage1\")\n",
    "global_step = 0\n",
    "num_updates = total_timesteps // steps_per_iter\n",
    "for update in range(1, num_updates+1):\n",
    "    traj = agent.collect_trajectory(env, steps_per_iter)\n",
    "    pg_loss, v_loss, ent_loss = agent.update(traj)\n",
    "    # Log training metrics\n",
    "    rewards = traj[\"rewards\"]; dones = traj[\"dones\"]\n",
    "    ep_returns = []\n",
    "    cum_reward = 0.0\n",
    "    for r, d in zip(rewards, dones):\n",
    "        cum_reward += r\n",
    "        if d:\n",
    "            ep_returns.append(cum_reward)\n",
    "            cum_reward = 0.0\n",
    "    if ep_returns:\n",
    "        writer.add_scalar(\"charts/episodic_return\", np.mean(ep_returns), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss, global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss, global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", ent_loss, global_step)\n",
    "    global_step += len(rewards)\n",
    "\n",
    "    if update == 1: start_time = time.time()\n",
    "    if update % 10 == 0:\n",
    "        avg_ret = np.mean(ep_returns) if ep_returns else 0.0\n",
    "        elapsed = time.time() - start_time\n",
    "        updates_done = update\n",
    "        updates_left = num_updates - updates_done\n",
    "        time_per_update = elapsed / updates_done\n",
    "        eta = updates_left * time_per_update\n",
    "        print(f\"Update {update}/{num_updates} [{int(100*update/num_updates):3d}%] | AvgReturn: {avg_ret:.2f} | ETA: {eta/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a481d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training PPO, collect trajectories to train distance models\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env = GoalObservationWrapper(eval_env)\n",
    "eval_env = TerminateOnSuccessWrapper(eval_env)\n",
    "eval_episodes = 100\n",
    "sup_states = []\n",
    "sup_distances = []      \n",
    "td_transitions = []\n",
    "success_count = 0\n",
    "for ep in range(eval_episodes):\n",
    "    state, _ = eval_env.reset()\n",
    "    ep_states = [state]\n",
    "    transitions = []\n",
    "    cum_reward = 0.0\n",
    "    step_count = 0\n",
    "    success = False\n",
    "    while True:\n",
    "        action, logp, val = agent.ac.act(state)  # use trained policy\n",
    "        next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        transitions.append((state, next_state, done, bool(info.get('success', False))))\n",
    "        cum_reward += reward\n",
    "        step_count += 1\n",
    "        state = next_state\n",
    "        ep_states.append(state)\n",
    "        if done:\n",
    "            success = info.get('success', False)\n",
    "            if success:\n",
    "                success_count += 1\n",
    "                # For each state in this successful episode, record true distance to goal\n",
    "                # If episode length = step_count, distance for state[i] = step_count - i\n",
    "                for i in range(step_count):\n",
    "                    sup_states.append(ep_states[i])\n",
    "                    sup_distances.append(step_count - i)\n",
    "                # Include the final goal state with distance 0\n",
    "                sup_states.append(ep_states[-1])\n",
    "                sup_distances.append(0.0)\n",
    "            # Add all transitions to TD dataset (failures will be handled in training)\n",
    "            td_transitions.extend(transitions)\n",
    "            break\n",
    "print(f\"Collected data from {eval_episodes} episodes, {success_count} were successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_states = np.array(sup_states, dtype=np.float32)\n",
    "sup_distances = np.array(sup_distances, dtype=np.float32)\n",
    "# Train distance estimators on the collected data\n",
    "sup_model = distance_models.SupervisedDistanceEstimator(input_dim=obs_dim)\n",
    "sup_loss = sup_model.train_from_data(sup_states, sup_distances, epochs=100)\n",
    "# td_model = distance_models.TDDistanceEstimator(input_dim=obs_dim)\n",
    "# td_loss = td_model.train_from_transitions(td_transitions, epochs=100)\n",
    "# Compare models on the supervised dataset\n",
    "sup_preds = sup_model.model(torch.tensor(sup_states)).detach().numpy().flatten()\n",
    "# td_preds = td_model.model(torch.tensor(sup_states)).detach().numpy().flatten()\n",
    "mse_sup = np.mean((sup_preds - sup_distances)**2)\n",
    "# mse_td = np.mean((td_preds - sup_distances)**2)\n",
    "print(f\"Supervised model MSE on training data: {mse_sup:.4f}\")\n",
    "# print(f\"TD model MSE on training data: {mse_td:.4f}\")\n",
    "# Save models for Stage 2\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "# torch.save(td_model.state_dict(), \"models/distance_model_td.pth\")\n",
    "torch.save(sup_model.state_dict(), \"models/distance_model_sup.pth\")\n",
    "torch.save(agent.ac.state_dict(), \"models/ppo_agent_stage1.pth\")\n",
    "# Record a video of the trained agent in PointMaze\n",
    "video_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "video_env = GoalObservationWrapper(video_env)\n",
    "video_env = TerminateOnSuccessWrapper(video_env)\n",
    "video_env = RecordVideo(video_env, video_folder=\"videos/stage1\", episode_trigger=lambda eid: True)\n",
    "vid_obs, _ = video_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _, _ = agent.ac.act(vid_obs)\n",
    "    vid_obs, _, terminated, truncated, info = video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "video_env.close()\n",
    "env.close(); eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbc0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
